{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kzwpkGM15Keo",
        "pk0PcTJV5Uap",
        "lcR-EqhH5eEY",
        "P66BRqD05tJl"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikio-bonjour/Python_code/blob/%E5%BE%AE%E5%88%86%E7%A9%8D%E5%88%86/%E5%8D%98%E7%B4%94%E3%83%8F%E3%82%9A%E3%83%BC%E3%82%BB%E3%83%95%E3%82%9A%E3%83%88%E3%83%AD%E3%83%B3%E3%81%AE%E5%AE%9F%E8%A3%85.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 単純パーセプトロンを 3 通りで実装\n",
        "- **パターン A** : 単純に Python (`math`)で計算\n",
        "- **パターン B** : NumPy でベクトル化\n",
        "- **パターン C** : PyTorch で層を宣言  \n",
        "それぞれで **ŷ ≈ 0.8808** の出力結果が得られるか確認します。"
      ],
      "metadata": {
        "id": "kzwpkGM15Keo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## パターン A — 単純に Python で計算\n",
        "- **重み付き和 → シグモイド** をそのまま書く\n",
        "- 手計算との 1:1 対応を確認するのが目的"
      ],
      "metadata": {
        "id": "pk0PcTJV5Uap"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cdr4ZVWZy3Ye",
        "outputId": "02804251-d6aa-4189-e2e6-d3d516140a05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "z = 2.0\n",
            "ŷ = 0.880797\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "def sigmoid(z: float) -> float:\n",
        "    \"\"\"\n",
        "  標準ロジスティックシグモイド活性化関数の定義\n",
        "    \"\"\"\n",
        "    return 1 / (1 + math.exp(-z))\n",
        "\n",
        "def perceptron(inputs, weights, activation=sigmoid):\n",
        "    \"\"\"\n",
        "    単一ニューロン（パーセプトロン）の出力を計算する。\n",
        "      inputs  : x0, x1, …  （バイアス項を含む）\n",
        "      weights : w0, w1, …\n",
        "      activation : 活性化関数（デフォルト: シグモイド）\n",
        "    戻り値は (線形結合 z, 出力 \\hat{y})\n",
        "    \"\"\"\n",
        "    z = sum(i * w for i, w in zip(inputs, weights))\n",
        "    return z, activation(z)\n",
        "\n",
        "x = [1.0, 2.0, -1.0]     # x0=1 (バイアス), x1=2, x2=-1\n",
        "w = [-1.0, 2.0, 1.0]     # w0=-1, w1=2, w2=1\n",
        "\n",
        "z_val, y_hat = perceptron(x, w)\n",
        "\n",
        "print(f\"z = {z_val}\")        # -> 2.0\n",
        "print(f\"ŷ = {y_hat:.6f}\")    # -> 0.880797\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## パターン B — NumPy でベクトル／バッチ対応\n",
        "- 行列積 `@` が **線形層** と同じ働きになる\n",
        "- 「バッチ（複数サンプル）」を一括処理する例で示す\n"
      ],
      "metadata": {
        "id": "lcR-EqhH5eEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([1.0, 2.0, -1.0])\n",
        "w = np.array([-1.0, 2.0, 1.0])\n",
        "\n",
        "z = x @ w\n",
        "y = 1 / (1 + np.exp(-z))\n",
        "\n",
        "# ─ バッチ計算例 ───────────────────\n",
        "X_batch = np.array([\n",
        "    [1.0, 2.0, -1.0],  # サンプル1\n",
        "    [0.5, 1.5, 2.0],   # サンプル2\n",
        "    [3.0, -1.0, 0.0]   # サンプル3\n",
        "])\n",
        "Z = X_batch @ w\n",
        "Y = 1 / (1 + np.exp(-Z))\n",
        "print(Y)"
      ],
      "metadata": {
        "id": "nJuSDQ3iy4UT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## パターン C — PyTorch で層を宣言\n",
        "- `nn.Linear` が **w·x + b**, `nn.Sigmoid` が活性化\n",
        "- Colab は PyTorch がプリインストール済み\n",
        "- 乱数初期化を避けるため **手動で重み・バイアスを設定* する"
      ],
      "metadata": {
        "id": "P66BRqD05tJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 手動で設定するパラメータ（重み w1, w2 と バイアス w0）\n",
        "w0, w1, w2 = -1.0, 2.0, 1.0\n",
        "\n",
        "# 入力データ x: 2次元特徴量を持つサンプルが1つだけある (batch=1, features=2)\n",
        "# ここではバイアス項は含まず、単純に2次元の実数値のみ\n",
        "x = torch.tensor([[2.0, -1.0]])  # shape: (1, 2)\n",
        "\n",
        "# nn.Sequential:\n",
        "# - まず nn.Linear(2, 1, bias=True) で入力2次元、出力1次元の線形変換を定義\n",
        "# - 続いて nn.Sigmoid() で活性化関数としてシグモイドを適用\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(2, 1, bias=True),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# with torch.no_grad(): を使うことで、以下の操作で\n",
        "# PyTorch が勾配を追跡しないようにする（トレーニング目的ではないので不要）\n",
        "with torch.no_grad():\n",
        "    # model[0] は nn.Sequential(...) の一番目の層 (nn.Linear(2, 1, bias=True))\n",
        "    # weight.copy_() で、重みの値を手動で [ [w1, w2] ] にコピー\n",
        "    # shape を (1,2) として対応させる\n",
        "    model[0].weight.copy_(torch.tensor([[w1, w2]]))\n",
        "\n",
        "    # bias.copy_() で、バイアスの値を手動で [w0] にコピー\n",
        "    # shape を (1,) として対応させる\n",
        "    model[0].bias.copy_(torch.tensor([w0]))\n",
        "\n",
        "# 設定したパラメータでモデルを実行する\n",
        "y_hat = model(x)\n",
        "\n",
        "# テンソル（単一スカラー）の値を Python の float として取得して表示\n",
        "print(y_hat.item())"
      ],
      "metadata": {
        "id": "JOiClA2J2RtP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}